{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T17:23:18.647645900Z",
     "start_time": "2024-07-16T17:23:04.576338900Z"
    }
   },
   "id": "b68b64d5b2a10449"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Environnement:\n",
    "    def nombre_etats(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def nombre_actions(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def recompense(self, etat, action, prochain_etat):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def probabilite_transition(self, etat, action, prochain_etat):\n",
    "        raise NotImplementedError\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T17:24:13.563127900Z",
     "start_time": "2024-07-16T17:24:13.534826300Z"
    }
   },
   "id": "bc61ce032715dc73"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class SecretEnv0(Environnement):\n",
    "    def __init__(self):\n",
    "        dll_path = r'C:\\Users\\rabia\\OneDrive\\Documents\\iabd\\deep renfo\\[Projet] Secret envs 0_1_ 2 and 3 Python wrapper 2024_06_18 (with intel Mac architecture)\\libs\\secret_envs.dll'\n",
    "        self.lib = ctypes.cdll.LoadLibrary(dll_path)\n",
    "        \n",
    "        self.lib.secret_env_0_num_states.argtypes = []\n",
    "        self.lib.secret_env_0_num_states.restype = ctypes.c_size_t\n",
    "        \n",
    "        self.lib.secret_env_0_num_actions.argtypes = []\n",
    "        self.lib.secret_env_0_num_actions.restype = ctypes.c_size_t\n",
    "        \n",
    "        self.lib.secret_env_0_reward.argtypes = [ctypes.c_size_t, ctypes.c_size_t, ctypes.c_size_t]\n",
    "        self.lib.secret_env_0_reward.restype = ctypes.c_float\n",
    "        \n",
    "        self.lib.secret_env_0_transition_probability.argtypes = [ctypes.c_size_t, ctypes.c_size_t, ctypes.c_size_t]\n",
    "        self.lib.secret_env_0_transition_probability.restype = ctypes.c_float\n",
    "        \n",
    "        self.nS = self.lib.secret_env_0_num_states()\n",
    "        self.nA = self.lib.secret_env_0_num_actions()\n",
    "        \n",
    "        self.P = self._construire_probabilites_transition()\n",
    "\n",
    "    def _construire_probabilites_transition(self):\n",
    "        P = {s: {a: []} for s in range(self.nS) for a in range(self.nA)}\n",
    "        \n",
    "        for s in range(self.nS):\n",
    "            for a in range(self.nA):\n",
    "                for prochain_s in range(self.nS):\n",
    "                    prob = self.lib.secret_env_0_transition_probability(s, a, prochain_s)\n",
    "                    if prob > 0:\n",
    "                        recompense = self.lib.secret_env_0_reward(s, a, prochain_s)\n",
    "                        termine = False  # Ajuster si l'environnement a des états terminaux\n",
    "                        P[s][a].append((prob, prochain_s, recompense, termine))\n",
    "        return P\n",
    "\n",
    "    def nombre_etats(self):\n",
    "        return self.nS\n",
    "\n",
    "    def nombre_actions(self):\n",
    "        return self.nA\n",
    "\n",
    "    def recompense(self, etat, action, prochain_etat):\n",
    "        return self.lib.secret_env_0_reward(etat, action, prochain_etat)\n",
    "\n",
    "    def probabilite_transition(self, etat, action, prochain_etat):\n",
    "        return self.lib.secret_env_0_transition_probability(etat, action, prochain_etat)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T17:24:14.673416100Z",
     "start_time": "2024-07-16T17:24:14.654024600Z"
    }
   },
   "id": "caf3080842ef0a13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SecretEnv1(Environnement):\n",
    "    def __init__(self):\n",
    "        dll_path = r'C:\\Users\\rabia\\OneDrive\\Documents\\iabd\\deep renfo\\[Projet] Secret envs 0_1_ 2 and 3 Python wrapper 2024_06_18 (with intel Mac architecture)\\libs\\secret_envs.dll'\n",
    "        self.lib = ctypes.cdll.LoadLibrary(dll_path)\n",
    "        \n",
    "        self.lib.secret_env_1_num_states.argtypes = []\n",
    "        self.lib.secret_env_1_num_states.restype = ctypes.c_size_t\n",
    "        \n",
    "        self.lib.secret_env_1_num_actions.argtypes = []\n",
    "        self.lib.secret_env_1_num_actions.restype = ctypes.c_size_t\n",
    "        \n",
    "        self.lib.secret_env_1_reward.argtypes = [ctypes.c_size_t, ctypes.c_size_t, ctypes.c_size_t]\n",
    "        self.lib.secret_env_1_reward.restype = ctypes.c_float\n",
    "        \n",
    "        self.lib.secret_env_1_transition_probability.argtypes = [ctypes.c_size_t, ctypes.c_size_t, ctypes.c_size_t]\n",
    "        self.lib.secret_env_1_transition_probability.restype = ctypes.c_float\n",
    "        \n",
    "        self.nS = self.lib.secret_env_1_num_states()\n",
    "        self.nA = self.lib.secret_env_1_num_actions()\n",
    "        \n",
    "        self.P = self._construire_probabilites_transition()\n",
    "\n",
    "    def _construire_probabilites_transition(self):\n",
    "        P = {s: {a: []} for s in range(self.nS) for a in range(self.nA)}\n",
    "        \n",
    "        for s in range(self.nS):\n",
    "            for a in range(self.nA):\n",
    "                for prochain_s in range(self.nS):\n",
    "                    prob = self.lib.secret_env_1_transition_probability(s, a, prochain_s)\n",
    "                    if prob > 0:\n",
    "                        recompense = self.lib.secret_env_1_reward(s, a, prochain_s)\n",
    "                        termine = False  # Ajuster si l'environnement a des états terminaux\n",
    "                        P[s][a].append((prob, prochain_s, recompense, termine))\n",
    "        return P\n",
    "    #POUR dynamic programming \n",
    "    def nombre_etats(self):\n",
    "        return self.nS\n",
    "\n",
    "    def nombre_actions(self):\n",
    "        return self.nA\n",
    "\n",
    "    def recompense(self, etat, action, prochain_etat):\n",
    "        return self.lib.secret_env_1_reward(etat, action, prochain_etat)\n",
    "\n",
    "    def probabilite_transition(self, etat, action, prochain_etat):\n",
    "        return self.lib.secret_env_1_transition_probability(etat, action, prochain_etat)\n",
    "    # Pour monte carlo\n",
    "    \n",
    "    # Monte Carlo and TD Methods related functions:\n",
    "    def state_id(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_2_state_id(self.instance)\n",
    "\n",
    "    def reset(self):\n",
    "        self.wrapper.lib.secret_env_2_reset(self.instance)\n",
    "\n",
    "    def display(self):\n",
    "        self.wrapper.lib.secret_env_2_display(self.instance)\n",
    "\n",
    "    def is_forbidden(self, action: int) -> int:\n",
    "        return self.wrapper.lib.secret_env_2_is_forbidden(self.instance, action)\n",
    "\n",
    "    def is_game_over(self) -> bool:\n",
    "        return self.wrapper.lib.secret_env_2_is_game_over(self.instance)\n",
    "\n",
    "    def available_actions(self) -> np.ndarray:\n",
    "        actions_len = self.wrapper.lib.secret_env_2_available_actions_len(self.instance)\n",
    "        actions_pointer = self.wrapper.lib.secret_env_2_available_actions(self.instance)\n",
    "        arr = np.ctypeslib.as_array(actions_pointer, (actions_len,))\n",
    "        arr_copy = np.copy(arr)\n",
    "        self.wrapper.lib.secret_env_2_available_actions_delete(actions_pointer, actions_len)\n",
    "        return arr_copy\n",
    "\n",
    "    def step(self, action: int):\n",
    "        self.wrapper.lib.secret_env_2_step(self.instance, action)\n",
    "\n",
    "    def score(self):\n",
    "        return self.wrapper.lib.secret_env_2_score(self.instance)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_random_state() -> 'SecretEnv2':\n",
    "        wrapper = SecretEnv2Wrapper()\n",
    "        instance = wrapper.lib.secret_env_2_from_random_state()\n",
    "        return SecretEnv2(wrapper, instance)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b666b3627b80921"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class IterationDePolitique:\n",
    "    def __init__(self, env, gamma=0.9, theta=1e-6):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.politique = np.ones([env.nombre_etats(), env.nombre_actions()]) / env.nombre_actions()\n",
    "        self.fonction_de_valeur = np.zeros(env.nombre_etats())\n",
    "    \n",
    "    def evaluation_de_politique(self):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for e in range(self.env.nombre_etats()):\n",
    "                v = self.fonction_de_valeur[e]\n",
    "                nouvelle_valeur = 0\n",
    "                for a in range(self.env.nombre_actions()):\n",
    "                    for proba, prochain_etat, recompense, termine in self.env.P[e][a]:\n",
    "                        nouvelle_valeur += self.politique[e, a] * proba * (recompense + self.gamma * self.fonction_de_valeur[prochain_etat])\n",
    "                self.fonction_de_valeur[e] = nouvelle_valeur\n",
    "                delta = max(delta, abs(v - nouvelle_valeur))\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "    \n",
    "    def amelioration_de_politique(self):\n",
    "        politique_stable = True\n",
    "        for e in range(self.env.nombre_etats()):\n",
    "            ancienne_action = np.argmax(self.politique[e])\n",
    "            valeurs_actions = np.zeros(self.env.nombre_actions())\n",
    "            for a in range(self.env.nombre_actions()):\n",
    "                for proba, prochain_etat, recompense, termine in self.env.P[e][a]:\n",
    "                    valeurs_actions[a] += proba * (recompense + self.gamma * self.fonction_de_valeur[prochain_etat])\n",
    "            meilleure_action = np.argmax(valeurs_actions)\n",
    "            self.politique[e] = np.eye(self.env.nombre_actions())[meilleure_action]\n",
    "            if ancienne_action != meilleure_action:\n",
    "                politique_stable = False\n",
    "        return politique_stable\n",
    "    \n",
    "    def iterer(self):\n",
    "        start_time = time.time()\n",
    "        while True:\n",
    "            self.evaluation_de_politique()\n",
    "            if self.amelioration_de_politique():\n",
    "                break\n",
    "        end_time = time.time()\n",
    "        return self.politique, self.fonction_de_valeur, end_time - start_time\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T17:24:16.870135700Z",
     "start_time": "2024-07-16T17:24:16.848222200Z"
    }
   },
   "id": "c16672c8be55891b"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def grid_search(env, gammas, thetas):\n",
    "    results = []\n",
    "    for gamma in gammas:\n",
    "        for theta in thetas:\n",
    "            iteration_de_politique = IterationDePolitique(env, gamma=gamma, theta=theta)\n",
    "            politique_optimale, fonction_de_valeur_optimale, exec_time = iteration_de_politique.iterer()\n",
    "            results.append((gamma, theta, exec_time))\n",
    "            print(f\"Gamma: {gamma}, Theta: {theta}, Time: {exec_time:.4f} seconds\")\n",
    "    return results\n",
    "\n",
    "# Définir les plages de valeurs pour gamma et theta\n",
    "gammas = [0.8, 0.9, 0.95, 0.99]\n",
    "thetas = [1e-4, 1e-5, 1e-6, 1e-7]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-16T17:24:19.043561Z",
     "start_time": "2024-07-16T17:24:19.037019800Z"
    }
   },
   "id": "657020c1944f96c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialiser l'environnement\n",
    "env0 = SecretEnv0()\n",
    "\n",
    "# Effectuer la recherche en grille\n",
    "results = grid_search(env0, gammas, thetas)\n",
    "\n",
    "# Convertir les résultats en un DataFrame pour une meilleure visualisation\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Gamma', 'Theta', 'ExecutionTime'])\n",
    "\n",
    "# Visualiser les résultats\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(results_df.pivot('Gamma', 'Theta', 'ExecutionTime'), annot=True, fmt=\".4f\", cmap=\"YlGnBu\")\n",
    "plt.title(\"Execution Time for Different Gamma and Theta Values\")\n",
    "plt.xlabel(\"Theta\")\n",
    "plt.ylabel(\"Gamma\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-07-16T17:24:21.351227200Z"
    }
   },
   "id": "db8ef4f2edc6843e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8e77fca2eae1f9ab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
