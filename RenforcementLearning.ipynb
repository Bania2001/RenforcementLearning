{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89303bb46563c59d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lib_path = r'C:\\Users\\rabia\\OneDrive\\Documents\\iabd\\deep renfo\\[Projet] Secret envs 0_1_ 2 and 3 Python wrapper 2024_06_18 (with intel Mac architecture)\\libs\\secret_envs.dll'\n",
    "\n",
    "class SecretEnv0Wrapper:\n",
    "    def __init__(self):\n",
    "        self.lib = ctypes.cdll.LoadLibrary(lib_path)\n",
    "\n",
    "        # MDP functions\n",
    "        self.lib.secret_env_0_num_states.argtypes = []\n",
    "        self.lib.secret_env_0_num_states.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_0_num_actions.argtypes = []\n",
    "        self.lib.secret_env_0_num_actions.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_0_num_rewards.argtypes = []\n",
    "        self.lib.secret_env_0_num_rewards.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_0_reward.argtypes = [ctypes.c_size_t, ctypes.c_size_t, ctypes.c_size_t]\n",
    "        self.lib.secret_env_0_reward.restype = ctypes.c_float\n",
    "\n",
    "        self.lib.secret_env_0_transition_probability.argtypes = [ctypes.c_size_t, ctypes.c_size_t, ctypes.c_size_t]\n",
    "        self.lib.secret_env_0_transition_probability.restype = ctypes.c_float\n",
    "\n",
    "        # Monte Carlo and TD Methods\n",
    "        self.lib.secret_env_0_new.argtypes = []\n",
    "        self.lib.secret_env_0_new.restype = ctypes.c_void_p\n",
    "\n",
    "        self.lib.secret_env_0_reset.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_0_reset.restype = None\n",
    "\n",
    "        self.lib.secret_env_0_display.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_0_display.restype = None\n",
    "\n",
    "        self.lib.secret_env_0_state_id.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_0_state_id.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_0_is_forbidden.argtypes = [ctypes.c_void_p, ctypes.c_size_t]\n",
    "        self.lib.secret_env_0_is_forbidden.restype = ctypes.c_bool\n",
    "\n",
    "        self.lib.secret_env_0_is_game_over.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_0_is_game_over.restype = ctypes.c_bool\n",
    "\n",
    "        self.lib.secret_env_0_available_actions.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_0_available_actions.restype = ctypes.POINTER(ctypes.c_size_t)\n",
    "\n",
    "        self.lib.secret_env_0_available_actions_len.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_0_available_actions_len.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_0_available_actions_delete.argtypes = [ctypes.POINTER(ctypes.c_size_t), ctypes.c_size_t]\n",
    "        self.lib.secret_env_0_available_actions_delete.restype = None\n",
    "\n",
    "        self.lib.secret_env_0_step.argtypes = [ctypes.c_void_p, ctypes.c_size_t]\n",
    "        self.lib.secret_env_0_step.restype = None\n",
    "\n",
    "        self.lib.secret_env_0_score.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_0_score.restype = ctypes.c_float\n",
    "\n",
    "        self.lib.secret_env_0_delete.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_0_delete.restype = None\n",
    "\n",
    "        self.lib.secret_env_0_from_random_state.argtypes = []\n",
    "        self.lib.secret_env_0_from_random_state.restype = ctypes.c_void_p\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SecretEnv0:\n",
    "    def __init__(self, wrapper=None, instance=None):\n",
    "        if wrapper is None:\n",
    "            wrapper = SecretEnv0Wrapper()\n",
    "        self.wrapper = wrapper\n",
    "        if instance is None:\n",
    "            instance = self.wrapper.lib.secret_env_0_new()\n",
    "        self.instance = instance\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.wrapper is not None:\n",
    "            self.wrapper.lib.secret_env_0_delete(self.instance)\n",
    "\n",
    "    # Méthodes liées au MDP\n",
    "    def num_states(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_0_num_states()\n",
    "\n",
    "    def num_actions(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_0_num_actions()\n",
    "\n",
    "    def num_rewards(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_0_num_rewards()\n",
    "\n",
    "    def reward(self, i: int) -> float:\n",
    "        return self.wrapper.lib.secret_env_0_reward(i)\n",
    "\n",
    "    def p(self, s: int, a: int, s_p: int, r_index: int) -> float:\n",
    "        return self.wrapper.lib.secret_env_0_transition_probability(s, a, s_p, r_index)\n",
    "\n",
    "    # Méthodes liées à Monte Carlo et TD\n",
    "    def state_id(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_0_state_id(self.instance)\n",
    "\n",
    "    def reset(self):\n",
    "        self.wrapper.lib.secret_env_0_reset(self.instance)\n",
    "\n",
    "    def display(self):\n",
    "        self.wrapper.lib.secret_env_0_display(self.instance)\n",
    "\n",
    "    def is_forbidden(self, action: int) -> int:\n",
    "        return self.wrapper.lib.secret_env_0_is_forbidden(self.instance, action)\n",
    "\n",
    "    def is_game_over(self) -> bool:\n",
    "        return self.wrapper.lib.secret_env_0_is_game_over(self.instance)\n",
    "\n",
    "    def available_actions(self) -> np.ndarray:\n",
    "        actions_len = self.wrapper.lib.secret_env_0_available_actions_len(self.instance)\n",
    "        actions_pointer = self.wrapper.lib.secret_env_0_available_actions(self.instance)\n",
    "        arr = np.ctypeslib.as_array(actions_pointer, (actions_len,))\n",
    "        arr_copy = np.copy(arr)\n",
    "        self.wrapper.lib.secret_env_0_available_actions_delete(actions_pointer, actions_len)\n",
    "        return arr_copy\n",
    "\n",
    "    def step(self, action: int):\n",
    "        self.wrapper.lib.secret_env_0_step(self.instance, action)\n",
    "\n",
    "    def score(self):\n",
    "        return self.wrapper.lib.secret_env_0_score(self.instance)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_random_state() -> 'SecretEnv0':\n",
    "        wrapper = SecretEnv0Wrapper()\n",
    "        instance = wrapper.lib.secret_env_0_from_random_state()\n",
    "        return SecretEnv0(wrapper, instance)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20a53733bd7e7885"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SecretEnv1Wrapper:\n",
    "    def __init__(self):\n",
    "        self.lib = ctypes.cdll.LoadLibrary(lib_path)\n",
    "\n",
    "        # MDP functions\n",
    "        self.lib.secret_env_1_num_states.argtypes = []\n",
    "        self.lib.secret_env_1_num_states.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_1_num_actions.argtypes = []\n",
    "        self.lib.secret_env_1_num_actions.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_1_num_rewards.argtypes = []\n",
    "        self.lib.secret_env_1_num_rewards.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_1_reward.argtypes = [ctypes.c_size_t, ctypes.c_size_t, ctypes.c_size_t]\n",
    "        self.lib.secret_env_1_reward.restype = ctypes.c_float\n",
    "\n",
    "        self.lib.secret_env_1_transition_probability.argtypes = [ctypes.c_size_t, ctypes.c_size_t, ctypes.c_size_t]\n",
    "        self.lib.secret_env_1_transition_probability.restype = ctypes.c_float\n",
    "\n",
    "        # Monte Carlo and TD Methods\n",
    "        self.lib.secret_env_1_new.argtypes = []\n",
    "        self.lib.secret_env_1_new.restype = ctypes.c_void_p\n",
    "\n",
    "        self.lib.secret_env_1_reset.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_1_reset.restype = None\n",
    "\n",
    "        self.lib.secret_env_1_display.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_1_display.restype = None\n",
    "\n",
    "        self.lib.secret_env_1_state_id.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_1_state_id.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_1_is_forbidden.argtypes = [ctypes.c_void_p, ctypes.c_size_t]\n",
    "        self.lib.secret_env_1_is_forbidden.restype = ctypes.c_bool\n",
    "\n",
    "        self.lib.secret_env_1_is_game_over.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_1_is_game_over.restype = ctypes.c_bool\n",
    "\n",
    "        self.lib.secret_env_1_available_actions.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_1_available_actions.restype = ctypes.POINTER(ctypes.c_size_t)\n",
    "\n",
    "        self.lib.secret_env_1_available_actions_len.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_1_available_actions_len.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_1_available_actions_delete.argtypes = [ctypes.POINTER(ctypes.c_size_t), ctypes.c_size_t]\n",
    "        self.lib.secret_env_1_available_actions_delete.restype = None\n",
    "\n",
    "        self.lib.secret_env_1_step.argtypes = [ctypes.c_void_p, ctypes.c_size_t]\n",
    "        self.lib.secret_env_1_step.restype = None\n",
    "\n",
    "        self.lib.secret_env_1_score.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_1_score.restype = ctypes.c_float\n",
    "\n",
    "        self.lib.secret_env_1_delete.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_1_delete.restype = None\n",
    "\n",
    "        self.lib.secret_env_1_from_random_state.argtypes = []\n",
    "        self.lib.secret_env_1_from_random_state.restype = ctypes.c_void_p\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2e53c21bae035c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SecretEnv1:\n",
    "    def __init__(self, wrapper=None, instance=None):\n",
    "        if wrapper is None:\n",
    "            wrapper = SecretEnv1Wrapper()\n",
    "        self.wrapper = wrapper\n",
    "        if instance is None:\n",
    "            instance = self.wrapper.lib.secret_env_1_new()\n",
    "        self.instance = instance\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.wrapper is not None:\n",
    "            self.wrapper.lib.secret_env_1_delete(self.instance)\n",
    "\n",
    "    # Méthodes liées au MDP\n",
    "    def num_states(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_1_num_states()\n",
    "\n",
    "    def num_actions(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_1_num_actions()\n",
    "\n",
    "    def num_rewards(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_1_num_rewards()\n",
    "\n",
    "    def reward(self, i: int) -> float:\n",
    "        return self.wrapper.lib.secret_env_1_reward(i)\n",
    "\n",
    "    def p(self, s: int, a: int, s_p: int, r_index: int) -> float:\n",
    "        return self.wrapper.lib.secret_env_1_transition_probability(s, a, s_p, r_index)\n",
    "\n",
    "    # Méthodes liées à Monte Carlo et TD\n",
    "    def state_id(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_1_state_id(self.instance)\n",
    "\n",
    "    def reset(self):\n",
    "        self.wrapper.lib.secret_env_1_reset(self.instance)\n",
    "\n",
    "    def display(self):\n",
    "        self.wrapper.lib.secret_env_1_display(self.instance)\n",
    "\n",
    "    def is_forbidden(self, action: int) -> int:\n",
    "        return self.wrapper.lib.secret_env_1_is_forbidden(self.instance, action)\n",
    "\n",
    "    def is_game_over(self) -> bool:\n",
    "        return self.wrapper.lib.secret_env_1_is_game_over(self.instance)\n",
    "\n",
    "    def available_actions(self) -> np.ndarray:\n",
    "        actions_len = self.wrapper.lib.secret_env_1_available_actions_len(self.instance)\n",
    "        actions_pointer = self.wrapper.lib.secret_env_1_available_actions(self.instance)\n",
    "        arr = np.ctypeslib.as_array(actions_pointer, (actions_len,))\n",
    "        arr_copy = np.copy(arr)\n",
    "        self.wrapper.lib.secret_env_1_available_actions_delete(actions_pointer, actions_len)\n",
    "        return arr_copy\n",
    "\n",
    "    def step(self, action: int):\n",
    "        self.wrapper.lib.secret_env_1_step(self.instance, action)\n",
    "\n",
    "    def score(self):\n",
    "        return self.wrapper.lib.secret_env_1_score(self.instance)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_random_state() -> 'SecretEnv1':\n",
    "        wrapper = SecretEnv1Wrapper()\n",
    "        instance = wrapper.lib.secret_env_1_from_random_state()\n",
    "        return SecretEnv1(wrapper, instance)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb0f1860327a8af3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SecretEnv2Wrapper:\n",
    "    def __init__(self):\n",
    "        self.lib = ctypes.cdll.LoadLibrary(lib_path)\n",
    "\n",
    "        # MDP functions\n",
    "        self.lib.secret_env_2_num_states.argtypes = []\n",
    "        self.lib.secret_env_2_num_states.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_2_num_actions.argtypes = []\n",
    "        self.lib.secret_env_2_num_actions.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_2_num_rewards.argtypes = []\n",
    "        self.lib.secret_env_2_num_rewards.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_2_reward.argtypes = [ctypes.c_size_t, ctypes.c_size_t, ctypes.c_size_t]\n",
    "        self.lib.secret_env_2_reward.restype = ctypes.c_float\n",
    "\n",
    "        self.lib.secret_env_2_transition_probability.argtypes = [ctypes.c_size_t, ctypes.c_size_t, ctypes.c_size_t]\n",
    "        self.lib.secret_env_2_transition_probability.restype = ctypes.c_float\n",
    "\n",
    "        # Monte Carlo and TD Methods\n",
    "        self.lib.secret_env_2_new.argtypes = []\n",
    "        self.lib.secret_env_2_new.restype = ctypes.c_void_p\n",
    "\n",
    "        self.lib.secret_env_2_reset.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_2_reset.restype = None\n",
    "\n",
    "        self.lib.secret_env_2_display.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_2_display.restype = None\n",
    "\n",
    "        self.lib.secret_env_2_state_id.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_2_state_id.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_2_is_forbidden.argtypes = [ctypes.c_void_p, ctypes.c_size_t]\n",
    "        self.lib.secret_env_2_is_forbidden.restype = ctypes.c_bool\n",
    "\n",
    "        self.lib.secret_env_2_is_game_over.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_2_is_game_over.restype = ctypes.c_bool\n",
    "\n",
    "        self.lib.secret_env_2_available_actions.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_2_available_actions.restype = ctypes.POINTER(ctypes.c_size_t)\n",
    "\n",
    "        self.lib.secret_env_2_available_actions_len.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_2_available_actions_len.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_2_available_actions_delete.argtypes = [ctypes.POINTER(ctypes.c_size_t), ctypes.c_size_t]\n",
    "        self.lib.secret_env_2_available_actions_delete.restype = None\n",
    "\n",
    "        self.lib.secret_env_2_step.argtypes = [ctypes.c_void_p, ctypes.c_size_t]\n",
    "        self.lib.secret_env_2_step.restype = None\n",
    "\n",
    "        self.lib.secret_env_2_score.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_2_score.restype = ctypes.c_float\n",
    "\n",
    "        self.lib.secret_env_2_delete.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_2_delete.restype = None\n",
    "\n",
    "        self.lib.secret_env_2_from_random_state.argtypes = []\n",
    "        self.lib.secret_env_2_from_random_state.restype = ctypes.c_void_p\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dce47ab4b9bc6aae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SecretEnv2:\n",
    "    def __init__(self, wrapper=None, instance=None):\n",
    "        if wrapper is None:\n",
    "            wrapper = SecretEnv2Wrapper()\n",
    "        self.wrapper = wrapper\n",
    "        if instance is None:\n",
    "            instance = self.wrapper.lib.secret_env_2_new()\n",
    "        self.instance = instance\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.wrapper is not None:\n",
    "            self.wrapper.lib.secret_env_2_delete(self.instance)\n",
    "\n",
    "    # Méthodes liées au MDP\n",
    "    def num_states(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_2_num_states()\n",
    "\n",
    "    def num_actions(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_2_num_actions()\n",
    "\n",
    "    def num_rewards(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_2_num_rewards()\n",
    "\n",
    "    def reward(self, i: int) -> float:\n",
    "        return self.wrapper.lib.secret_env_2_reward(i)\n",
    "\n",
    "    def p(self, s: int, a: int, s_p: int, r_index: int) -> float:\n",
    "        return self.wrapper.lib.secret_env_2_transition_probability(s, a, s_p, r_index)\n",
    "\n",
    "    # Méthodes liées à Monte Carlo et TD\n",
    "    def state_id(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_2_state_id(self.instance)\n",
    "\n",
    "    def reset(self):\n",
    "        self.wrapper.lib.secret_env_2_reset(self.instance)\n",
    "\n",
    "    def display(self):\n",
    "        self.wrapper.lib.secret_env_2_display(self.instance)\n",
    "\n",
    "    def is_forbidden(self, action: int) -> int:\n",
    "        return self.wrapper.lib.secret_env_2_is_forbidden(self.instance, action)\n",
    "\n",
    "    def is_game_over(self) -> bool:\n",
    "        return self.wrapper.lib.secret_env_2_is_game_over(self.instance)\n",
    "\n",
    "    def available_actions(self) -> np.ndarray:\n",
    "        actions_len = self.wrapper.lib.secret_env_2_available_actions_len(self.instance)\n",
    "        actions_pointer = self.wrapper.lib.secret_env_2_available_actions(self.instance)\n",
    "        arr = np.ctypeslib.as_array(actions_pointer, (actions_len,))\n",
    "        arr_copy = np.copy(arr)\n",
    "        self.wrapper.lib.secret_env_2_available_actions_delete(actions_pointer, actions_len)\n",
    "        return arr_copy\n",
    "\n",
    "    def step(self, action: int):\n",
    "        self.wrapper.lib.secret_env_2_step(self.instance, action)\n",
    "\n",
    "    def score(self):\n",
    "        return self.wrapper.lib.secret_env_2\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9707b058f83577bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SecretEnv3Wrapper:\n",
    "    def __init__(self):\n",
    "        self.lib = ctypes.cdll.LoadLibrary(lib_path)\n",
    "\n",
    "        # MDP functions\n",
    "        self.lib.secret_env_3_num_states.argtypes = []\n",
    "        self.lib.secret_env_3_num_states.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_3_num_actions.argtypes = []\n",
    "        self.lib.secret_env_3_num_actions.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_3_num_rewards.argtypes = []\n",
    "        self.lib.secret_env_3_num_rewards.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_3_reward.argtypes = [ctypes.c_size_t, ctypes.c_size_t, ctypes.c_size_t]\n",
    "        self.lib.secret_env_3_reward.restype = ctypes.c_float\n",
    "\n",
    "        self.lib.secret_env_3_transition_probability.argtypes = [ctypes.c_size_t, ctypes.c_size_t, ctypes.c_size_t]\n",
    "        self.lib.secret_env_3_transition_probability.restype = ctypes.c_float\n",
    "\n",
    "        # Monte Carlo and TD Methods\n",
    "        self.lib.secret_env_3_new.argtypes = []\n",
    "        self.lib.secret_env_3_new.restype = ctypes.c_void_p\n",
    "\n",
    "        self.lib.secret_env_3_reset.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_3_reset.restype = None\n",
    "\n",
    "        self.lib.secret_env_3_display.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_3_display.restype = None\n",
    "\n",
    "        self.lib.secret_env_3_state_id.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_3_state_id.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_3_is_forbidden.argtypes = [ctypes.c_void_p, ctypes.c_size_t]\n",
    "        self.lib.secret_env_3_is_forbidden.restype = ctypes.c_bool\n",
    "\n",
    "        self.lib.secret_env_3_is_game_over.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_3_is_game_over.restype = ctypes.c_bool\n",
    "\n",
    "        self.lib.secret_env_3_available_actions.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_3_available_actions.restype = ctypes.POINTER(ctypes.c_size_t)\n",
    "\n",
    "        self.lib.secret_env_3_available_actions_len.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_3_available_actions_len.restype = ctypes.c_size_t\n",
    "\n",
    "        self.lib.secret_env_3_available_actions_delete.argtypes = [ctypes.POINTER(ctypes.c_size_t), ctypes.c_size_t]\n",
    "        self.lib.secret_env_3_available_actions_delete.restype = None\n",
    "\n",
    "        self.lib.secret_env_3_step.argtypes = [ctypes.c_void_p, ctypes.c_size_t]\n",
    "        self.lib.secret_env_3_step.restype = None\n",
    "\n",
    "        self.lib.secret_env_3_score.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_3_score.restype = ctypes.c_float\n",
    "\n",
    "        self.lib.secret_env_3_delete.argtypes = [ctypes.c_void_p]\n",
    "        self.lib.secret_env_3_delete.restype = None\n",
    "\n",
    "        self.lib.secret_env_3_from_random_state.argtypes = []\n",
    "        self.lib.secret_env_3_from_random_state.restype = ctypes.c_void_p\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95a1a8ce19da2b17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SecretEnv3:\n",
    "    def __init__(self, wrapper=None, instance=None):\n",
    "        if wrapper is None:\n",
    "            wrapper = SecretEnv3Wrapper()\n",
    "        self.wrapper = wrapper\n",
    "        if instance is None:\n",
    "            instance = self.wrapper.lib.secret_env_3_new()\n",
    "        self.instance = instance\n",
    "\n",
    "    def __del__(self):\n",
    "        if self.wrapper is not None:\n",
    "            self.wrapper.lib.secret_env_3_delete(self.instance)\n",
    "\n",
    "    # Méthodes liées au MDP\n",
    "    def num_states(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_3_num_states()\n",
    "\n",
    "    def num_actions(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_3_num_actions()\n",
    "\n",
    "    def num_rewards(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_3_num_rewards()\n",
    "\n",
    "    def reward(self, i: int) -> float:\n",
    "        return self.wrapper.lib.secret_env_3_reward(i)\n",
    "\n",
    "    def p(self, s: int, a: int, s_p: int, r_index: int) -> float:\n",
    "        return self.wrapper.lib.secret_env_3_transition_probability(s, a, s_p, r_index)\n",
    "\n",
    "    # Méthodes liées à Monte Carlo et TD\n",
    "    def state_id(self) -> int:\n",
    "        return self.wrapper.lib.secret_env_3_state_id(self.instance)\n",
    "\n",
    "    def reset(self):\n",
    "        self.wrapper.lib.secret_env_3_reset(self.instance)\n",
    "\n",
    "    def display(self):\n",
    "        self.wrapper.lib.secret_env_3_display(self.instance)\n",
    "\n",
    "    def is_forbidden(self, action: int) -> int:\n",
    "        return self.wrapper.lib.secret_env_3_is_forbidden(self.instance, action)\n",
    "\n",
    "    def is_game_over(self) -> bool:\n",
    "        return self.wrapper.lib.secret_env_3_is_game_over(self.instance)\n",
    "\n",
    "    def available_actions(self) -> np.ndarray:\n",
    "        actions_len = self.wrapper.lib.secret_env_3_available_actions_len(self.instance)\n",
    "        actions_pointer = self.wrapper.lib.secret_env_3_available_actions(self.instance)\n",
    "        arr = np.ctypeslib.as_array(actions_pointer, (actions_len,))\n",
    "        arr_copy = np.copy(arr)\n",
    "        self.wrapper.lib.secret_env_3_available_actions_delete(actions_pointer, actions_len)\n",
    "        return arr_copy\n",
    "\n",
    "    def step(self, action: int):\n",
    "        self.wrapper.lib.secret_env_3_step(self.instance, action)\n",
    "\n",
    "    def score(self):\n",
    "        return self.wrapper.lib.secret_env_3_score(self.instance)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_random_state() -> 'SecretEnv3':\n",
    "        wrapper = SecretEnv3Wrapper()\n",
    "        instance = wrapper.lib.secret_env_3_from_random_state()\n",
    "        return SecretEnv3(wrapper, instance)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fb9475e00eb7ca2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "PolicyIteration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8767098d998b789"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "class IterationDePolitique:\n",
    "    def __init__(self, env, gamma=0.9, theta=1e-6):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.politique = np.ones([env.num_states(), env.num_actions()]) / env.num_actions()\n",
    "        self.fonction_de_valeur = np.zeros(env.num_states())\n",
    "    \n",
    "    def evaluation_de_politique(self):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for e in range(self.env.num_states()):\n",
    "                v = self.fonction_de_valeur[e]\n",
    "                nouvelle_valeur = 0\n",
    "                for a in range(self.env.num_actions()):\n",
    "                    for proba, prochain_etat, recompense, termine in self.env.P[e][a]:\n",
    "                        nouvelle_valeur += self.politique[e, a] * proba * (recompense + self.gamma * self.fonction_de_valeur[prochain_etat])\n",
    "                self.fonction_de_valeur[e] = nouvelle_valeur\n",
    "                delta = max(delta, abs(v - nouvelle_valeur))\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "    \n",
    "    def amelioration_de_politique(self):\n",
    "        politique_stable = True\n",
    "        for e in range(self.env.num_states()):\n",
    "            ancienne_action = np.argmax(self.politique[e])\n",
    "            valeurs_actions = np.zeros(self.env.num_actions())\n",
    "            for a in range(self.env.num_actions()):\n",
    "                for proba, prochain_etat, recompense, termine in self.env.P[e][a]:\n",
    "                    valeurs_actions[a] += proba * (recompense + self.gamma * self.fonction_de_valeur[prochain_etat])\n",
    "            meilleure_action = np.argmax(valeurs_actions)\n",
    "            self.politique[e] = np.eye(self.env.num_actions())[meilleure_action]\n",
    "            if ancienne_action != meilleure_action:\n",
    "                politique_stable = False\n",
    "        return politique_stable\n",
    "    \n",
    "    def iterer(self):\n",
    "        start_time = time.time()\n",
    "        while True:\n",
    "            self.evaluation_de_politique()\n",
    "            if self.amelioration_de_politique():\n",
    "                break\n",
    "        end_time = time.time()\n",
    "        return self.politique, self.fonction_de_valeur, end_time - start_time\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20842ed495c823fc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pour aplliquer plicy itération\n",
    "def grid_search_pi(env, gammas, thetas):\n",
    "    results = []\n",
    "    for gamma in gammas:\n",
    "        for theta in thetas:\n",
    "            iteration_de_politique = IterationDePolitique(env, gamma=gamma, theta=theta)\n",
    "            politique_optimale, fonction_de_valeur_optimale, exec_time = iteration_de_politique.iterer()\n",
    "            results.append((gamma, theta, exec_time))\n",
    "            print(f\"Gamma: {gamma}, Theta: {theta}, Time: {exec_time:.4f} seconds\")\n",
    "    return results\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c894477f2a2fb9ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Définir les plages de valeurs pour gamma et theta\n",
    "gammas = [0.8, 0.9, 0.95, 0.99]\n",
    "thetas = [1e-4, 1e-5, 1e-6, 1e-7]\n",
    "\n",
    "# Initialiser les environnements\n",
    "env0 = SecretEnv0()\n",
    "env1 = SecretEnv1()\n",
    "env2 = SecretEnv2()\n",
    "env3 = SecretEnv3()\n",
    "\n",
    "# Effectuer la recherche en grille pour chaque environnement\n",
    "print(\"Recherche en grille pour SecretEnv0:\")\n",
    "results_env0 = grid_search_pi(env0, gammas, thetas)\n",
    "\n",
    "print(\"\\nRecherche en grille pour SecretEnv1:\")\n",
    "results_env1 = grid_search_pi(env1, gammas, thetas)\n",
    "\n",
    "print(\"\\nRecherche en grille pour SecretEnv2:\")\n",
    "results_env2 = grid_search_pi(env2, gammas, thetas)\n",
    "\n",
    "print(\"\\nRecherche en grille pour SecretEnv3:\")\n",
    "results_env3 = grid_search_pi(env3, gammas, thetas)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48794229b63dd25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(results, env_name):\n",
    "    df = pd.DataFrame(results, columns=['Gamma', 'Theta', 'ExecutionTime'])\n",
    "    pivot_table = df.pivot('Gamma', 'Theta', 'ExecutionTime')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(pivot_table, annot=True, fmt=\".4f\", cmap=\"YlGnBu\")\n",
    "    plt.title(f\"Temps d'exécution pour {env_name} avec différentes valeurs de Gamma et Theta\")\n",
    "    plt.xlabel(\"Theta\")\n",
    "    plt.ylabel(\"Gamma\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualiser les résultats pour chaque environnement\n",
    "plot_results(results_env0, \"SecretEnv0\")\n",
    "plot_results(results_env1, \"SecretEnv1\")\n",
    "plot_results(results_env2, \"SecretEnv2\")\n",
    "plot_results(results_env3, \"SecretEnv3\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee5983296f21f9fb"
  },
  {
   "cell_type": "raw",
   "source": [
    "Value Ieration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ed85dbab46c3bfb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class IterationDeValeur:\n",
    "    def __init__(self, env, gamma=0.9, theta=1e-6):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.fonction_de_valeur = np.zeros(env.num_states())\n",
    "        self.politique = np.zeros(env.num_states(), dtype=int)\n",
    "    \n",
    "    def iteration_de_valeur(self):\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for s in range(self.env.num_states()):\n",
    "                v = self.fonction_de_valeur[s]\n",
    "                valeurs_actions = np.zeros(self.env.num_actions())\n",
    "                for a in range(self.env.num_actions()):\n",
    "                    for prob, prochain_etat, recompense, termine in self.env.P[s][a]:\n",
    "                        valeurs_actions[a] += prob * (recompense + self.gamma * self.fonction_de_valeur[prochain_etat])\n",
    "                self.fonction_de_valeur[s] = np.max(valeurs_actions)\n",
    "                delta = max(delta, abs(v - self.fonction_de_valeur[s]))\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "    \n",
    "    def extraire_politique(self):\n",
    "        for s in range(self.env.num_states()):\n",
    "            valeurs_actions = np.zeros(self.env.num_actions())\n",
    "            for a in range(self.env.num_actions()):\n",
    "                for prob, prochain_etat, recompense, termine in self.env.P[s][a]:\n",
    "                    valeurs_actions[a] += prob * (recompense + self.gamma * self.fonction_de_valeur[prochain_etat])\n",
    "            self.politique[s] = np.argmax(valeurs_actions)\n",
    "        return self.politique\n",
    "    \n",
    "    def iterer(self):\n",
    "        start_time = time.time()\n",
    "        self.iteration_de_valeur()\n",
    "        politique = self.extraire_politique()\n",
    "        end_time = time.time()\n",
    "        return politique, self.fonction_de_valeur, end_time - start_time\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7be8cac4c4607e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def grid_search_vi(env, gammas, thetas):\n",
    "    results = []\n",
    "    for gamma in gammas:\n",
    "        for theta in thetas:\n",
    "            iteration_de_valeur = IterationDeValeur(env, gamma=gamma, theta=theta)\n",
    "            politique_optimale, fonction_de_valeur_optimale, exec_time = iteration_de_valeur.iterer()\n",
    "            results.append((gamma, theta, exec_time))\n",
    "            print(f\"Gamma: {gamma}, Theta: {theta}, Time: {exec_time:.4f} seconds\")\n",
    "    return results\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c98937fdefb7e216"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Effectuer la recherche en grille pour chaque environnement\n",
    "print(\"Recherche en grille pour Value Iteration sur SecretEnv0:\")\n",
    "results_vi_env0 = grid_search_vi(env0, gammas, thetas)\n",
    "\n",
    "print(\"\\nRecherche en grille pour Value Iteration sur SecretEnv1:\")\n",
    "results_vi_env1 = grid_search_vi(env1, gammas, thetas)\n",
    "\n",
    "print(\"\\nRecherche en grille pour Value Iteration sur SecretEnv2:\")\n",
    "results_vi_env2 = grid_search_vi(env2, gammas, thetas)\n",
    "\n",
    "print(\"\\nRecherche en grille pour Value Iteration sur SecretEnv3:\")\n",
    "results_vi_env3 = grid_search_vi(env3, gammas, thetas)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77f3acc168feb43a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualiser les résultats pour chaque environnement\n",
    "plot_results(results_vi_env0, \"Value Iteration - SecretEnv0\")\n",
    "plot_results(results_vi_env1, \"Value Iteration - SecretEnv1\")\n",
    "plot_results(results_vi_env2, \"Value Iteration - SecretEnv2\")\n",
    "plot_results(results_vi_env3, \"Value Iteration - SecretEnv3\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "248881cbeeb09f11"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
